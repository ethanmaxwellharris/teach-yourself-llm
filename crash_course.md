ðŸ§  Tier 1: Core Foundations
[Attention is All You Need](https://arxiv.org/abs/1706.03762)

[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

[Improving Language Understanding by Generative Pre-training (GPT-1)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

[The Illustrated Transformer (visual guide by Jay Alammar)](https://jalammar.github.io/illustrated-transformer/)

ðŸ§° Tier 2: From Models to Applications
[Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)

[Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165)

[InstructGPT: Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)

[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)

[Parameter-Efficient Transfer Learning for NLP (Adapters)](https://arxiv.org/abs/1902.00751)

ðŸ§± Tier 3: Architectural Mastery
[Transformer Feedforward Layers Are Key-Value Memories](https://arxiv.org/abs/2006.16236)

[A Mechanistic Interpretability Analysis of Grokking](https://transformer-circuits.pub/2023/grokking/index.html)

[Training Compute-Optimal Large Language Models (Chinchilla)](https://arxiv.org/abs/2203.15556)

ðŸš€ Tier 4: Cutting-Edge & Directional
[Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/abs/2302.04761)

[Retentive Network: A Successor to Transformer for Large Language Models](https://arxiv.org/abs/2307.08621)

[LLaMA 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)

[Anthropic Interpretability & Claude Research (multiple)](https://www.anthropic.com/research)

[Evaluating Verifiability in Generative Models (OpenAI)](https://openai.com/research/evaluating-verifiability-in-generative-models)
